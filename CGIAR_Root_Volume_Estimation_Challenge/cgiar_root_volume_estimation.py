# -*- coding: utf-8 -*-
"""cgiar_root_volume_estimation_challenge_github_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r0uMVgw4kYlvxePAd02UTK2pNkiuarbk

Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

""" Installing Required Packages"""

!pip install ultralytics optuna category_encoders feature-engine scikeras

"""Environment Setup and Data Preparation"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Environment Setup and Data Preparation
# -----------------------------------------------

# Standard Libraries
import os
from types import SimpleNamespace

# Data Manipulation and Analysis
import numpy as np
import pandas as pd

# Image Processing
import cv2
from PIL import Image
from skimage import measure
from skimage.morphology import skeletonize
from skimage.feature import graycomatrix, graycoprops, local_binary_pattern

# Machine Learning and Deep Learning
import torch
from torchvision import models, transforms
from ultralytics import YOLO
import ultralytics

# Visualization
import matplotlib.pyplot as plt

# Machine Learning Utilities
from sklearn.model_selection import train_test_split, RepeatedKFold, TimeSeriesSplit, cross_val_score
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import (
    StandardScaler, QuantileTransformer, PolynomialFeatures, OneHotEncoder
)
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.pipeline import Pipeline

# Optimization
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner
from optuna import Trial

# Advanced Machine Learning Libraries
from xgboost import XGBRegressor, callback
from lightgbm import LGBMRegressor

# Signal Processing
from scipy.signal import savgol_filter

# File and Directory Operations
import zipfile
import glob


# %matplotlib inline

""" Data Extraction and CSV Loading"""

# Extract Director
extract_dir = '/content/drive/MyDrive/CGIR'


# The CSV files are stored inside the data/ directory.
train_csv_path = os.path.join(extract_dir, 'Train.csv')
test_csv_path  = os.path.join(extract_dir, 'Test.csv')

df_train = pd.read_csv(train_csv_path)
df_test  = pd.read_csv(test_csv_path)

#Drop Zero RootVolume Rows
df_train = df_train[df_train["RootVolume"] != 0].copy()

df_test

df_train['PlantID'] = df_train['ID']
df_test['PlantID'] = df_test['ID']

"""Unzipping the Main Dataset and Models"""

# ----- Unzip the main dataset (data.zip) -----
data_zip_path = '/content/drive/MyDrive/CGIR/data.zip'
data_extract_dir = '/content/data'  # This will be our root folder for the dataset

if not os.path.exists(data_extract_dir):
    print("Extracting dataset from", data_zip_path)
    with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:
        zip_ref.extractall(data_extract_dir)
    print("Dataset extraction complete!")
else:
    print("Dataset already extracted in", data_extract_dir)

# ----- Unzip the models (Models.zip) -----
models_zip_path = '/content/drive/MyDrive/CGIR/Models.zip'
models_extract_dir = '/content/Models'  # This folder should be created after extraction

if not os.path.exists(models_extract_dir):
    print("Extracting models from", models_zip_path)
    with zipfile.ZipFile(models_zip_path, 'r') as zip_ref:
        zip_ref.extractall()  # Extracts to current directory; creates Models/ folder
    print("Models extraction complete!")
else:
    print("Models already extracted in", models_extract_dir)

"""Listing and Verifying Dataset Structure"""

# List top-level folders in the dataset (should be 'train' and 'test')
top_folders = [d for d in os.listdir(data_extract_dir) if os.path.isdir(os.path.join(data_extract_dir, d))]
print("Top-level folders found in dataset:", top_folders)

# For each top-level folder, list unique subfolders and count images within them.
for top_folder in top_folders:
    top_folder_path = os.path.join(data_extract_dir, top_folder)
    subfolders = [d for d in os.listdir(top_folder_path) if os.path.isdir(os.path.join(top_folder_path, d))]
    print(f"\nFolder '{top_folder}' contains {len(subfolders)} subfolders:")
    for subfolder in subfolders:
        subfolder_path = os.path.join(top_folder_path, subfolder)
        images = glob.glob(os.path.join(subfolder_path, '*.png'))
        print(f"  Subfolder '{subfolder}': {len(images)} images")

"""Visualizing Sample Images"""

# ------------------------------------------------------------------------------
# Step 3: Visualize Sample Images
# ------------------------------------------------------------------------------

def show_image(img_path, title=None,width=5,height=5):
    """Load and display an image given its path."""
    img = cv2.imread(img_path)
    if img is None:
        print("Image not found:", img_path)
        return
    # Convert BGR (OpenCV default) to RGB for display.
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(width, height))
    plt.imshow(img_rgb)
    if title:
        plt.title(title)
    plt.axis('off')
    plt.show()

# For demonstration, select a sample subfolder from the 'train' directory.
sample_train_dir = os.path.join(data_extract_dir, 'train')
sample_subfolders = [d for d in os.listdir(sample_train_dir) if os.path.isdir(os.path.join(sample_train_dir, d))]
if sample_subfolders:
    sample_subfolder = sample_subfolders[0]
    sample_subfolder_path = os.path.join(sample_train_dir, sample_subfolder)
    # Construct sample image names using the naming convention:
    #   {subfolder}_{S}_{layer:03d}.png
    sample_left_image = os.path.join(sample_subfolder_path, f"{sample_subfolder}_L_001.png")
    sample_right_image = os.path.join(sample_subfolder_path, f"{sample_subfolder}_R_001.png")
    print("Displaying sample images from subfolder:", sample_subfolder)
    show_image(sample_left_image, "Sample Left Scan (Layer 001)")
    show_image(sample_right_image, "Sample Right Scan (Layer 001)")
else:
    print("No subfolders found in train directory.")

"""Model Loading Function"""

# ----------------------------
# Model Loading
# ----------------------------

def load_segmentation_model(model_path):
    """Load YOLOv8 segmentation model"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    return YOLO(model_path)

# Initialize models
model_e = load_segmentation_model(os.path.join(models_extract_dir, 'best_early.pt'))
model_l = load_segmentation_model(os.path.join(models_extract_dir, 'best_late.pt'))

# ----------------------------
# Core Segmentation Functions
# ----------------------------

def segment_roots(image_path, model):
    """Robust root segmentation with mask combination"""
    img = cv2.imread(image_path)
    if img is None:
        return np.zeros((640, 640), dtype=np.uint8)

    results = model.predict(img, conf=0.3, imgsz=640, verbose=False)

    if not results or not results[0].masks:
        return np.zeros(img.shape[:2], dtype=np.uint8)

    combined_mask = np.any(results[0].masks.data.cpu().numpy() > 0.5, axis=0)
    return combined_mask.astype(np.uint8) * 255

def calculate_area(mask):
    """Calculate valid root area with noise filtering"""
    if mask is None or np.sum(mask) < 100:  # Ignore small noisy areas
        return 0
    return np.sum(mask > 0)

# ----------------------------
# Feature Extraction Functions
# ----------------------------
def morphological_features(mask):
    """Extract morphological features with segment-wise analysis"""
    mask_bin = (mask > 0).astype(np.uint8)
    features = {}

    # Skeleton analysis
    skeleton = skeletonize(mask_bin)
    features['SkeletonLength'] = np.sum(skeleton)

    # Initialize region analysis
    labeled_mask = measure.label(mask_bin)
    regions = measure.regionprops(labeled_mask)

    # Thickness and shape analysis
    thickness_values = []
    eccentricities = []
    area_values = []

    for region in regions:
        if region.area >= 100:  # Match noise threshold
            # Thickness calculation
            segment_mask = (labeled_mask == region.label)
            segment_skeleton = skeletonize(segment_mask)
            skel_length = np.sum(segment_skeleton)
            if skel_length > 0:
                thickness_values.append(region.area / skel_length)

            # Shape characteristics
            eccentricities.append(region.eccentricity)
            area_values.append(region.area)

    # Thickness features
    features['AvgThickness'] = np.sum(mask_bin)/features['SkeletonLength'] if features['SkeletonLength'] > 0 else 0
    features['MeanThickness'] = np.mean(thickness_values) if thickness_values else 0

    # Shape features
    features['MeanShapeEccentricity'] = np.mean(eccentricities) if eccentricities else 0
    features['MeanShapeArea'] = np.mean(area_values) if area_values else 0
    # Contour analysis
    contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        main_contour = max(contours, key=cv2.contourArea)
        features['ContourArea'] = cv2.contourArea(main_contour)
        features['ContourPerimeter'] = cv2.arcLength(main_contour, True)
        features['ContourComplexity'] = features['ContourPerimeter']**2 / (4*np.pi*features['ContourArea']) if features['ContourArea'] > 0 else 0

        # Calculate mean shape area of significant contours
        all_areas = [cv2.contourArea(cnt) for cnt in contours]
        significant_areas = [a for a in all_areas if a >= 100]  # Match area threshold from calculate_area()
        features['MeanShapeArea'] = np.mean(significant_areas) if significant_areas else 0
    else:
        features.update({
            'ContourArea': 0,
            'ContourPerimeter': 0,
            'ContourComplexity': 0,
            'MeanShapeArea': 0
        })

    return features

#--------------------------------------
# Texture Features
#--------------------------------------
def texture_features(mask):
    """Enhanced texture analysis with segment-wise GLCM"""
    mask_bin = (mask > 0).astype(np.uint8)
    features = {}

    # Initialize region analysis
    labeled_mask = measure.label(mask_bin)
    regions = measure.regionprops(labeled_mask)

    # GLCM features per segment
    contrast_values = []
    homogeneity_values = []

    for region in regions:
        if region.area >= 100:
            # Extract segment image
            minr, minc, maxr, maxc = region.bbox
            segment_img = mask_bin[minr:maxr, minc:maxc]

            # Calculate GLCM properties
            glcm = graycomatrix(segment_img, [1], [0], symmetric=True, normed=True)
            contrast_values.append(graycoprops(glcm, 'contrast')[0, 0])
            homogeneity_values.append(graycoprops(glcm, 'homogeneity')[0, 0])

    # Texture means
    features['MeanTextureContrast'] = np.mean(contrast_values) if contrast_values else 0
    features['MeanTextureHomogeneity'] = np.mean(homogeneity_values) if homogeneity_values else 0

    # LBP analysis
    img = (mask_bin * 255).astype(np.uint8)
    lbp = local_binary_pattern(img, P=8, R=1, method='uniform')
    features['MeanLBP'] = np.mean(lbp)

    # GLCM features
    glcm = graycomatrix(img, [1], [0], symmetric=True, normed=True)
    features['GLCM_Contrast'] = graycoprops(glcm, 'contrast')[0, 0]
    features['GLCM_Correlation'] = graycoprops(glcm, 'correlation')[0, 0]
    features['GLCM_Energy'] = graycoprops(glcm, 'energy')[0, 0]
    features['GLCM_Homogeneity'] = graycoprops(glcm, 'homogeneity')[0, 0]

    # LBP features
    lbp = local_binary_pattern(img, P=8, R=1, method='uniform')
    hist, _ = np.histogram(lbp, bins=np.arange(0, 10), density=True)
    features.update({f'LBP_{i}': hist[i] for i in range(9)})

    return features

def fractal_dimension(mask):
    """Box-counting fractal dimension with corrected implementation"""
    def boxcount(z, k):
        """Properly nested reduceat calls"""
        # Reduce along rows first
        row_reduced = np.add.reduceat(z, np.arange(0, z.shape[0], k), axis=0)
        # Then reduce along columns
        return np.add.reduceat(row_reduced, np.arange(0, z.shape[1], k), axis=1)

    mask_bin = (mask > 0).astype(int)
    if 0 in mask_bin.shape or np.sum(mask_bin) == 0:
        return 0.0

    try:
        max_size = int(np.log2(min(mask_bin.shape)))
        sizes = 2**np.arange(max_size, 1, -1)
    except ValueError:
        return 0.0

    counts = []
    for size in sizes:
        if size >= min(mask_bin.shape):
            continue
        bc = boxcount(mask_bin, size)
        counts.append(np.sum(bc > 0))

    if len(counts) < 2:
        return 0.0

    # Safely handle log calculations
    with np.errstate(divide='ignore', invalid='ignore'):
        x = np.log(sizes[:len(counts)])
        y = np.log(np.array(counts) + 1e-10)
        coeffs = np.polyfit(x, y, 1)

    return float(-coeffs[0])

# ----------------------------
# mean fractal dimension
# ----------------------------
def mean_fractal_dimension(mask):
    """Calculate mean fractal dimension across root segments"""
    mask_bin = (mask > 0).astype(int)
    labeled = measure.label(mask_bin)
    regions = measure.regionprops(labeled)

    fd_values = []
    for region in regions:
        if region.area >= 100:
            segment_mask = (labeled == region.label).astype(np.uint8)
            fd = fractal_dimension(segment_mask)
            fd_values.append(fd)

    return np.mean(fd_values) if fd_values else 0


# ----------------------------
# Deep Feature Extraction
# ----------------------------

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet = models.resnet18(pretrained=True).eval().to(device)
feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])
deep_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def deep_features(image_path):
    """Extract deep features from pretrained ResNet"""
    try:
        img = Image.open(image_path).convert('RGB')
        img_tensor = deep_transform(img).unsqueeze(0).to(device)
        with torch.no_grad():
            features = feature_extractor(img_tensor)
        return features.cpu().numpy().flatten()
    except:
        return np.zeros(512)  # Fallback for ResNet18 features

# ----------------------------
# Modified Stereo Processing
# ----------------------------

def process_stereo_pair(left_path, right_path):
    """Updated stereo processing with deep feature averaging"""
    # Disparity calculation
    disparity = compute_disparity_sgbm(left_path, right_path)

    features = {'Disparity': disparity}
    deep_values = []

    # Process left view
    if left_path and os.path.exists(left_path):
        left_deep = deep_features(left_path)
        features.update({f'Left_{i}': v for i, v in enumerate(left_deep)})
        deep_values.extend(left_deep)

    # Process right view
    if right_path and os.path.exists(right_path):
        right_deep = deep_features(right_path)
        features.update({f'Right_{i}': v for i, v in enumerate(right_deep)})
        deep_values.extend(right_deep)

    # Calculate mean of all deep features
    features['MeanDeepFeature'] = np.mean(deep_values) if deep_values else 0

    return features


def compute_disparity_sgbm(left_path, right_path):
    """Robust disparity calculation with auto-scaling"""
    left = cv2.imread(left_path, 0)
    right = cv2.imread(right_path, 0)

    if left is None or right is None:
        return 0

    # Auto-resize to smallest common size
    h, w = min(left.shape[0], right.shape[0]), min(left.shape[1], right.shape[1])
    left = cv2.resize(left, (w, h))
    right = cv2.resize(right, (w, h))

    # Configure stereo matcher
    window_size = 5
    matcher = cv2.StereoSGBM_create(
        minDisparity=0,
        numDisparities=64,
        blockSize=window_size,
        P1=8*3*window_size**2,
        P2=32*3*window_size**2,
        disp12MaxDiff=1,
        uniquenessRatio=10,
        speckleWindowSize=100,
        speckleRange=32
    )

    disparity = matcher.compute(left, right).astype(np.float32)/16
    return np.mean(disparity[disparity > 0]) if np.any(disparity > 0) else 0



def calculate_3d_metrics(disparities, areas):
    """Calculate voxel-based 3D features"""
    voxel_volume = np.sum(areas) * np.mean(disparities)
    surface_area = np.sum([np.sqrt(area) for area in areas])  # Approximate surface area
    return {
        'VoxelVolume': voxel_volume,
        'SurfaceArea3D': surface_area,
        'Compactness3D': voxel_volume / (surface_area + 1e-6)
    }


# ----------------------------
# Complete Feature Estimation
# ----------------------------
def extract_plant_features(folder_path, plant_info):
    """Complete feature extraction pipeline"""
    features = {}
    layer_features = []
    disparities = []

    for layer in range(plant_info['Start'], plant_info['End'] + 1):
        left_path = os.path.join(folder_path, f"{plant_info['FolderName']}_L_{layer:03d}.png")
        right_path = os.path.join(folder_path, f"{plant_info['FolderName']}_R_{layer:03d}.png")

        # Select appropriate model
        model = model_e if plant_info['Stage'] == 'Early' else model_l

        # Process left view
        left_mask = segment_roots(left_path, model) if os.path.exists(left_path) else None
        left_area = calculate_area(left_mask) if left_mask is not None else 0

        # Process right view
        right_mask = segment_roots(right_path, model) if os.path.exists(right_path) else None
        right_area = calculate_area(right_mask) if right_mask is not None else 0

        # Layer statistics
        layer_area = left_area + right_area

        # Get the best mask (view with higher area)
        best_mask = left_mask if left_area > right_area else right_mask

        # Calculate features
        morpho_features = morphological_features(best_mask)
        texture_features_result = texture_features(best_mask)
        fractal_dim = fractal_dimension(best_mask)
        mean_fractal_dim = mean_fractal_dimension(best_mask)

        # Create layer entry
        layer_entry = {
            'Layer': layer,
            'TotalArea': layer_area,
            **morpho_features,
            **texture_features_result,
            'FractalDimension': fractal_dim,
            'MeanFractalDimension': mean_fractal_dim
        }

        # Process stereo features
        if os.path.exists(left_path) and os.path.exists(right_path):
            stereo_features = process_stereo_pair(left_path, right_path)
            disparities.append(stereo_features.pop('Disparity'))
            layer_entry.update(stereo_features)
        else:
            # Handle single-view deep features
            deep_vals = []
            if os.path.exists(left_path):
                deep_vals.extend(deep_features(left_path))
            if os.path.exists(right_path):
                deep_vals.extend(deep_features(right_path))
            layer_entry['MeanDeepFeature'] = np.mean(deep_vals) if deep_vals else 0

        layer_features.append(layer_entry)

    # Handle case with no valid layers
    if not layer_features:
        return features

    # Calculate temporal features
    areas = [lf['TotalArea'] for lf in layer_features]
    disparities_array = np.array(disparities)

    # Add to feature aggregation: 3D Spatial Features
    features.update(calculate_3d_metrics(disparities_array, areas))

    # Smooth area progression and calculate derivatives
    smoothed_areas = savgol_filter(areas, window_length=5, polyorder=2)
    features.update({
        'AreaGrowthRate': np.mean(np.diff(smoothed_areas)),
        'AreaAcceleration': np.mean(np.diff(smoothed_areas, 2))
    })

    # Calculate volume
    volume = 0
    if len(areas) > 1:
        thickness = np.abs(np.diff(disparities_array)) if len(disparities_array) > 1 else np.ones(len(areas)-1)
        if np.mean(thickness) > 0:
            thickness = thickness / np.mean(thickness)
        volume = sum((areas[i] + areas[i+1])/2 * thickness[min(i, len(thickness)-1)]
                   for i in range(len(areas)-1))

    # Aggregate features
    features.update({
        'EstimatedVolume': volume,
        'TotalArea': sum(areas),
        'MaxLayerArea': max(areas) if areas else 0,
        'AreaVariance': np.var(areas) if areas else 0,
        'MeanDisparity': np.mean(disparities_array) if disparities_array.size > 0 else 0,
        'DisparityRange': np.ptp(disparities_array) if disparities_array.size > 0 else 0,
        'NumActiveLayers': sum(a > 0 for a in areas),
        **{k: np.mean([lf[k] for lf in layer_features]) for k in layer_features[0].keys() if k != 'Layer'}
    })

    # Add metadata
    features.update({
        'Genotype': plant_info['Genotype'],
        'Stage': plant_info['Stage'],
        'PlantID': plant_info['PlantID'],
    })
    if 'RootVolume' in plant_info:
        features['RootVolume'] = plant_info['RootVolume']

    return features


# ----------------------------
# Main Processing Loop
# ----------------------------

def process_all_plants(df, data_dir):
    """Process all plants in the dataframe"""
    results = []

    for idx, row in df.iterrows():
        subfolder = find_subfolder_path(row['FolderName'], data_dir)
        if subfolder is None:
            continue

        try:
            features = extract_plant_features(subfolder, row)
            results.append(features)
        except Exception as e:
            print(f"Error processing process_all_plants {row['FolderName']}: {str(e)}")

    return pd.DataFrame(results).fillna(0)

# Utility function to locate image folders
def find_subfolder_path(folder_name, base_dir):
    for root, dirs, _ in os.walk(base_dir):
        if folder_name in dirs:
            return os.path.join(root, folder_name)
    return None

# Process all plants
feature_df = process_all_plants(df_train, '/content/data/train/')

""" MIN ON GOOGLE COLAB T4 GPU"""

# Process all plants
feature_test_df = process_all_plants(df_test, '/content/data/test/')

feature_df

def create_interaction_features(df):
    """Advanced feature engineering with validation checks"""
    # Create copy to avoid modifying original dataframe
    df = df.copy()

    # Check for required base columns
    required_columns = [
        'EstimatedVolume', 'MeanShapeArea', 'MeanTextureContrast',
        'MeanTextureHomogeneity', 'MeanShapeEccentricity',
        'MeanFractalDimension', 'MeanLBP', 'MeanThickness',
        'MeanDeepFeature', 'AvgThickness'
    ]

    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")

    try:
        # Biological interaction terms
        df['Volumetric_Density'] = df['EstimatedVolume'] / (df['MeanShapeArea'].replace(0, 1e-6))
        df['Texture_Complexity'] = df['MeanTextureContrast'] * df['MeanTextureHomogeneity']
        df['MorphoBalance'] = (df['MeanShapeEccentricity'] * df['MeanFractalDimension']) / df['MeanLBP'].replace(0, 1e-6)

        # Polynomial features with validation
        poly = PolynomialFeatures(
            degree=3,
            interaction_only=True,
            include_bias=False
        )

        # Ensure numerical stability
        poly_input = df[['MeanThickness', 'MeanDeepFeature', 'AvgThickness']].fillna(0)
        poly_features = poly.fit_transform(poly_input)

        # Create feature names and prevent column collisions
        poly_cols = [
            f"Poly_{name}"
            for name in poly.get_feature_names_out(['MeanThickness', 'MeanDeepFeature', 'AvgThickness'])
        ]

        df_poly = pd.DataFrame(poly_features, columns=poly_cols, index=df.index)

        return pd.concat([df, df_poly], axis=1)

    except Exception as e:
        print(f"Feature engineering failed: {str(e)}")
        return df  # Return original dataframe on failure

# Usage example:
# df_engineered = create_interaction_features(your_dataframe)

train_data_df = create_interaction_features(feature_df)

train_data_df

test_data_df = create_interaction_features(feature_test_df)

test_data_df

"""Advanced Feature Engineering Function"""

def create_advanced_features(df, target_col=None, is_test=False, fit_quantile=None):
    """
    Competition-grade feature engineering with enhanced encoding and biological feature creation.

    Parameters:
        df (pd.DataFrame): Input DataFrame
        target_col (str): Target column name (required for encoding)
        is_test (bool): Whether processing test data
        fit_quantile (object): Pre-fit QuantileTransformer for test data

    Returns:
        pd.DataFrame, object: Engineered DataFrame and fit QuantileTransformer
    """
    df = df.copy()

    # ================== COMPETITION-GRADE ENCODING ==================
    # Enhanced Stage encoding with cyclical features
    if 'Stage' in df.columns:
        df['Stage'] = df['Stage'].map({'Early': 0, 'Late': 1}).fillna(-1)
        df['Stage_sin'] = np.sin(2 * np.pi * df['Stage'] / 24)
        df['Stage_cos'] = np.cos(2 * np.pi * df['Stage'] / 24)

    # Multi-strategy Genotype encoding
    if 'Genotype' in df.columns:
        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        ohe.fit(df[['Genotype']].astype(str))
        ohe_features = pd.DataFrame(ohe.transform(df[['Genotype']].astype(str)),
                               columns=ohe.get_feature_names_out(['Genotype']))
        df = pd.concat([df, ohe_features], axis=1)

    # ================== QUANTILE TRANSFORMATION ==================
    qt_features = ['TotalArea', 'MeanThickness', 'MeanDeepFeature', 'MeanShapeArea']
    if not is_test:
        # Changed output distribution to 'uniform' to ensure non-negative values
        qt = QuantileTransformer(n_quantiles=200, output_distribution='uniform', random_state=42)
        df[qt_features] = qt.fit_transform(df[qt_features])
    else:
        df[qt_features] = fit_quantile.transform(df[qt_features])

    # ================== ADVANCED BIOLOGICAL FEATURES ==================
    interaction_features = {
        'MorphoTextureDepth': ['MeanThickness', 'GLCM_Contrast', 'MeanDeepFeature'],
        'VolumetricComplexity': ['EstimatedVolume', 'ContourComplexity', 'MeanFractalDimension'],
        'GrowthEfficiency': ['TotalArea', 'NumActiveLayers', 'MeanThickness']
    }

    for name, features in interaction_features.items():
        if all(f in df.columns for f in features):
            df[name] = df[features].prod(axis=1)
            # Ensure values are non-negative before log1p
            df[name] = df[name].clip(lower=0)  # Clip negative values to 0
            df[f'log_{name}'] = np.log1p(df[name] + 1e-6)

    # ================== DOMAIN-SPECIFIC RATIOS ==================
    biological_ratios = {
        'RootDensity': ('SkeletonLength', 'TotalArea'),
        'VolumeEfficiency': ('EstimatedVolume', 'MeanShapeArea'),
        'StructureQuality': ('ContourComplexity', 'MeanFractalDimension')
    }

    for name, (num, den) in biological_ratios.items():
        if num in df.columns and den in df.columns:
            df[name] = df[num] / (df[den] + 1e-6)
            # Clip ratios to avoid values <= -1
            df[name] = df[name].clip(lower=-1 + 1e-6)  # Ensure x >= -1 + epsilon
            df[f'{name}_log'] = np.log1p(df[name])

    # ================== TEMPORAL-DEPTH FEATURES ==================
    depth_features = ['TotalArea', 'MeanDisparity', 'NumActiveLayers']
    for feat in depth_features:
        if feat in df.columns:
            df[f'{feat}_DepthGradient'] = df.groupby('PlantID')[feat].transform(
                lambda x: x.diff().fillna(0) / (x.index.to_series().diff().fillna(1))
            )

    # ================== FINAL OPTIMIZATION ==================
    df = df.drop(columns=['Genotype', 'PlantID'], errors='ignore')

    if not is_test:
        selector = VarianceThreshold(threshold=0.0001) #changed from 0.1 = 1.091397254, 0.01 =1.071242355, 0.001 = 0.997195275, 0.0001 =
        df = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])

    df = df.fillna(0).replace([np.inf, -np.inf], 0)

    return df if is_test else (df, qt)

"""Applying Advanced Feature Engineering to Training and Test Data"""

# For training data
train_features, fitted_qt = create_advanced_features(
    train_data_df,
    target_col='RootVolume'
)

# For test data
test_features = create_advanced_features(
    test_data_df,
    is_test=True,
    fit_quantile=fitted_qt
)

# Final datasets
X = train_features.drop('RootVolume', axis=1)
y = train_features['RootVolume']
X_test = test_features

train_features

y

#Apply log transformation of RootVolume column
y = np.log1p(y)

y

X

#Make Sure X_test has same arrangement of columns as X dataframe
X_test = X_test[X.columns]

X_test

"""Enhanced Optuna Optimization Setup

"""

# ================== ENHANCED OPTUNA OPTIMIZATION WITH TPE ==================


def create_optuna_study(X, y, n_trials=100):
    studies = {}


    # 2. Optimize XGBoost with Early Stopping
    def xgb_objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'subsample': trial.suggest_float('subsample', 0.7, 0.95),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),
            'gamma': trial.suggest_float('gamma', 0, 5),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),
            'device': 'cuda',
        }
        model = XGBRegressor(**params, early_stopping_rounds=100, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

        # Create an EarlyStopping callback that stops training if there's no improvement in 100 rounds.
        #early_stopping_callback = EarlyStopping(rounds=100)

        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            #callbacks=[early_stopping_callback],
            verbose=False
        )

        # Return the square root of the best evaluation score
        return np.sqrt(model.best_score)

    # Create and run the Optuna study with the modified objective
    xgb_study = optuna.create_study(
        direction='minimize',
        sampler=TPESampler(seed=42, multivariate=True),
        study_name="XGBoost_TPE"
    )
    xgb_study.optimize(xgb_objective, n_trials=n_trials, show_progress_bar=True)
    studies['xgb'] = xgb_study

    # 1. Optimize Random Forest with Pruning
    def rf_objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),
            'max_depth': trial.suggest_int('max_depth', 5, 20),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),
            'max_features': trial.suggest_float('max_features', 0.2, 0.8),
            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
            #'max_samples': trial.suggest_float('max_samples', 0.6, 0.95) if params['bootstrap'] else None
        }
        # Define max_samples after params['bootstrap'] is assigned
        params['max_samples'] = trial.suggest_float('max_samples', 0.6, 0.95) if params['bootstrap'] else None

        model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)
        cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)
        scores = cross_val_score(model, X, y, cv=cv,
                               scoring='neg_mean_squared_error', n_jobs=-1)
        return np.mean(-scores)

    rf_study = optuna.create_study(
        direction='minimize',
        sampler=TPESampler(seed=42, n_startup_trials=20),
        pruner=MedianPruner(n_startup_trials=10),
        study_name="RandomForest_TPE"
    )
    rf_study.optimize(rf_objective, n_trials=n_trials, show_progress_bar=True)
    studies['rf'] = rf_study


#     return studies

"""Ensemble Building Function"""

############# Ensemble Building Function

def build_enhanced_ensemble(studies, X, y):
    """Build optimized ensemble with tuned meta-learner"""
    base_models = {
        'rf': RandomForestRegressor(**studies['rf'].best_params, random_state=42, n_jobs=-1),
        'xgb': XGBRegressor(**studies['xgb'].best_params, random_state=42,n_jobs=-1)
    }

    # Generate cross-validated meta-features
    meta_features, meta_y = generate_meta_features(base_models, X, y)

    # Optimize meta-model parameters to get finest results
    # meta_study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))
    # meta_study.optimize(lambda trial: meta_objective(trial, meta_features, meta_y),
    #                    n_trials=50, show_progress_bar=True)#n_trials=50
    #meta_study.best_params
    # Train final meta-model with best params
    params = {'n_estimators': 400, 'learning_rate': 0.02824023478063409, 'max_depth': 8, 'subsample': 0.8864932256799045, 'colsample_bytree': 0.9931189064936389, 'gamma': 0.14984437984307442, 'reg_alpha': 1.0974749960525234, 'reg_lambda': 4.547229832827324}
    best_meta = XGBRegressor(**params, early_stopping_rounds=120, random_state=42, device='cuda', n_jobs=-1)

    # Split meta-features into training and validation sets
    X_train_meta, X_val_meta, y_train_meta, y_val_meta = train_test_split(
        meta_features, meta_y, test_size=0.2, random_state=42
    )

    # Now fit the meta-model with eval_set
    best_meta.fit(X_train_meta, y_train_meta, eval_set=[(X_val_meta, y_val_meta)], verbose=False)

    return {'base_models': base_models, 'meta_model': best_meta}

# Meta-Feature Generation and Objective for Meta-Model

def generate_meta_features(base_models, X, y):
    """Time-series aware meta-feature generation"""
    tscv = TimeSeriesSplit(n_splits=10)
    meta_features = np.zeros((X.shape[0], len(base_models)))

    for train_idx, test_idx in tscv.split(X):
        for i, (name, model) in enumerate(base_models.items()):
            model.fit(X.iloc[train_idx], y.iloc[train_idx])
            meta_features[test_idx, i] = model.predict(X.iloc[test_idx])

    return meta_features, y

def meta_objective(trial: Trial, X_meta, y_meta):
    """Optuna objective for meta-model optimization"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),
        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 2, 12),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'device': 'cuda',
    }

    model = XGBRegressor(**params,random_state=42, n_jobs=-1)
    scores = cross_val_score(model, X_meta, y_meta,
                           cv=TimeSeriesSplit(n_splits=3),
                           scoring='neg_mean_squared_error',
                           n_jobs=-1)
    return np.mean(-scores)

"""Submission Generation Function"""

# ================== POST-PROCESSING & SUBMISSION ==================
def generate_submission(ensemble, X_test, df_test):
    # Generate predictions
    meta_features = np.column_stack([
        model.predict(X_test) for name, model in ensemble['base_models'].items()
    ])
    raw_preds = ensemble['meta_model'].predict(meta_features)

    # Post-processing steps
    predictions = pd.DataFrame({
        'ID': df_test['ID'],
        'RootVolume': np.expm1(raw_preds)  # Reverse log1p transformation
    })

    # Apply biological constraints
    predictions['RootVolume'] = predictions['RootVolume'].clip(lower=0)

    return predictions

# ================== FULL PIPELINE ==================
# 1. Train models with enhanced configuration
#studies = create_optuna_study(X, y, n_trials=200)
#final_ensemble = build_enhanced_ensemble(studies, X, y)

"""Full Pipeline and Ensemble Execution"""

### # For training data - These hyperparameters where collected from an optuna trial run. I found these to work better, i think there is still some variance in the code that needs to be reduced.
studiess = {
    'xgb': SimpleNamespace(best_params={ # Checked
        'n_estimators': 1962,
        'learning_rate': 0.15068276112066842,
        'max_depth': 7,
        'subsample': 0.7363230920349849,
        'colsample_bytree': 0.7677676295652908,
        'gamma': 0.1759430726857962,
        'reg_alpha': 0.40387005629111455,
        'reg_lambda': 1.3886309070185914,
        #'device': 'cuda',

      }),
    'rf': SimpleNamespace(best_params={
        'n_estimators': 100,
        'max_depth': 5,
        'min_samples_split': 8,
        'min_samples_leaf': 3,
        'max_features': 0.5973508225806464,
        'bootstrap': True,
        'max_samples': 0.6911095892085732

      })
}

#Build ensemble function
final_ensemble = build_enhanced_ensemble(studiess, X, y)

# 2. Generate and save predictions
submission_df = generate_submission(final_ensemble, X_test, df_test)
submission_df[['ID', 'RootVolume']].to_csv('best-score.csv', index=False)

print("Submission file created with post-processed predictions!")